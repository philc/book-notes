* Introduction
  * Argues these problems are the symptoms of "messes". Messes result from the inherent structure of
    complex systems:
    * "Hunger, poverty, environmental degradation, economic instability, unemployment... no one
      deliberately creates those problems, no one wants them to persist, but they persist
      nonetheless."
  * "Words and sentences must, by necessity, come only one at a time in linear, logical order.
    Systems happen all at once. They are connected not just in one direction, but in many directions
    simultaneously."
    * (so pictures and graphs must be used. This book has many systems diagrams, which look like
      UML diagrams).
  * Systems theory is a complementary lens to the observant human eye, and the detail-revealing
    microscope. It's not a superior lens. "The more ways of seeing, the better."
* The basics (chap 1)
  * System: elements interconnected to achieve a function. E.g. a digestive system.
  * It's easy to see the leements of a system. It's harder to see and understand the
    interconnections -- the physical flows, or flows of information, between elements.
  * Idiom: "function" is used for nonhuman systems, "purpose" for human systems.
  * "Systems can be nested within systems. Therefore, there can be purposes within purposes."
    * E.g. "the university", which is composed of each population / stakeholder group within it.
  * Changing elements -- e.g. the players on the team -- usually has the least effect on the
    systems' behavior. Changing interconnections, and function, usually have much larger effects.
  * Stocks: foundation of a system. These are elements that are a store, a measurable quantity. The
    quantities need not be physical; "accumulated goodwill" is a stock.
  * Flows: "Stocks change over time through the action of a flow. Flows are filling and raining,
    births and deaths, purchases and sales, growth and decay, deposits and withdrawals."
  * If the sum of inflows is equal to the sum of outflows of a stock, the stock's level remains
    constant.
  * For a labor force, you can increase its rate of growth by increasing hiring (inflow), or by
    reducing churn (outflow).
  * "A stock takes time to change, because flows take time to flow. That' sa vital point, a key to
    understanding why systems behave as they do."
  * "Stocks generally change slowly, even when the flows into or out of them change suddenly.
    Therefore, stocks act as delays or buffers or shock absorbers in systems."
  * The delay in changing stocks is why it takes so long for a business to gain traction. The stocks
    of employees, customers, and word of mouth accumulate slowly.
  * Inflows and outflows are made independent when a stock is used as a buffer in the system. It
    adds stability and predictability. Oil field -> oil reserves -> gas at pump.
  * "Systems thinkers see the world as a collection of stocks along with the mechanisms for
    regulating the levels in the stocks by manipulating flows."
  * Feedback loops
    * Stabilizing loops
      * A loop which tends to keep a stock's value within a range.
      * E.g. a coffee drinker's stock of energy will be maintained at a desired level by drinking or
        avoiding caffeine.
    * Runaway loops -- reinforcing feedback
      * Also called "snowballing" feedback loops.
      * "It generates more input to a stock the more that is already there (and less input the less
        that is already there). A reinforcing feedback loop enhances whatever direction of change is
        imposed on it."
      * Self-reinforcing: stocks with exponential growth properties. They can increase a constant
        percentage of their own value, so absolute change in value keeps growing with time. Population growth,
        for example.
  * Doubling time: the time it takes for an exponentially growing stock to double in size is 70 /
    growth rate.
  * "You'll stop looking for who's to blame; instead you'll start asking, 'What's the system?' The
    concept of feedback opens up the idea that a system can cause its own behavior."
* A brief visit to the systems zoo (chap 2)
  * In thermostat-like systems, one must take into account whatever draining or filling processes
    are affecting the stock. In a furnace for instance, the room's temperature is a result of
    furnace heat and heat loss to the outside, and as the furnace heats the room, it increases the
    rate of heat loss to the outside.
  * "A stock with one reinforcing loop and one balancing loop: population and industrial economy"
    * One loop in the system may be stronger / "dominant." Which loop is stronger may change with
      time ("shifting dominance"), as fertility and mortality rates do in population.
    * In a population system, the population stock stabilizes when fertility equals mortality, and
      the system reaches an equilibrium.
    * The investment -> capital stock -> depreciation system has similar structure as a population
      system.
  * "A system with delays: business inventory"
    * E.g. changing the staffing level in a company.
    * (Diagrams of these systems)
    * Lengthening and shortening delays using policy levers have powerful effects on the feedback
      and oscillations in output.
    * "Economies are extremely complex systems; they are full of balancing feedback loops with
      delays, and they are inherently oscillatory."
      * This is the cause of business cycles. The economy is a more complex version of the car
        dealership trying to maintain its inventory in relation to sales and manufacturing delays.
  * "A renewable stock constrained by a nonrenewable stock -- an oil economy"
    * In such systems, there is a reinforcing loop (e.g. exponential production adoption) and
      constraining loop (market saturation).
    * "In physical, exponentially growing systems, there must be at least one reinforcing loop
      driving the growth and at least one balancing loop constraining the growth, because no
      physical system can grow forever in a finite environment."
    * In a fish harvesting system, equilibrium is achieved when the harvest rate is equal to the
      natural replenishment rate. If the harvesting rate is larger, then the fish will be
      over-harvested and eventually eradicate all of the fish, turning this renewable resource into
      a non-renewable one.
* Why systems work so well (chap 3)
  * Resilience: "the ability to bounce or spring back into shape, position, etc., after being
    pressed or stretched. Elasticity. The ability to recover strength, spirits, good humor, or any
    other aspect quickly."
  * "Resilience is a measure of a system's ability to survive and persist within a variable
    environment. The opposite of resilience is brittleness or rigidity."
  * Feedback loops provide resilience in that they restore a system or set of stocks to their desired
    state after a large perturbation. Feedback loops which can create new feedback loops provide
    meta-resilience.
  * "Because resilience may not be obvious without a whole-system view, people often sacrifice
    resilience for stability, or for productivity, or for some other more immediately recognizable
    system property."
  * Self-organizing systems
    * Evolutionary systems, like life forms and societies, are hard to predict and model.
    * Self-organization produces heterogeneity and unpredictability. It is likely to come up with
      whole new structures, whole new ways of doing things. It requires freedom and experimentation,
      and a certain amount of disorder."
    * Self-organizing systems generate hierarchy. There are subsystems, aggregated into larger
      systems.
    * Balanced hierarchy: "there must be enough central control to achieve coordination toward the
      large-system goal, and enough autonomy to keep all subsystems flourishing, functioning, and
      self-organizing."
* Why systems surprise us (chap 4)
  * (An inventory of ways our mental models often fail to capture the complexity of reality.)
  * "The acquisition of knowledge always involves the revelation of ignorance -- almost *is* the
    revelation of ignorance. Our knowledge of the world instructs us first of all that the world is
    greater than our knowledge of it." - Wendell Berry
  * "Especially complex and sophisticated are the mental models we develop from direct, intimate
    experience of nature, people, and organizations immediately around us." These are superior to
    conjectured models.
  * "When a systems thinker encounters a problem, the first thing he or she does is look for data,
    time graphs, the history of the sy stem. That's because long-term behavior provides clues to the
    underlying system structure. ANd structure is the key to understanding not just *what* is
    happening, but *why*."
    * "Systems thinking goes back and forth constantly between structure (diagrams of stocks, flows,
      and feedback) and behavior (time graphs)."
  * Cool case study of budworm outbreaks, which consume fir trees. Outbreaks happen periodically
    because the system slowly oscillates between less fir trees and more, based on how long since
    last outbreak. Once fir trees hit a critical mass, there's a non-linearity: worm larvae multiply
    faster than natural predators can grow to consume them, and their population explodes, until
    they eat and reduce the fir tree population.
    * This is a "system with unintuitive nonlinearities."
  * A system may be constrained by any number of input factors.
    * "Rich countries transfer capital or technology to poor ones and wonder why the economies of
      the receiving countries still don't develop, never thinking that capital or technology may not
      be the most limiting factors."
  * Layers of limits (bottlenecks)
    * "There are layers of limits around every growing plant, child, epidemic, new product,
      technological advance, company, city, economy, and population. Insight comes not only from
      recognizing which factor is limiting, but from seeing that growth itself depletes or enhances
      limits and therefore changes what is limiting."
    * E.g. hiring more salespeople may shift the bottleneck to engineering.
  * Bounded rationality: humans in a system have incomplete information, and imperfect competence:
    we underestimate risk, have bias, interpret our information incorrectly. So no actor optimally
    acts for his own self good, or the good of the system."
